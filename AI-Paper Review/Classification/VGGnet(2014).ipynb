{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGGnet(2014).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMu26wrbmY7PXUgM7rM7Cmi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seonghoon-Yu/paper-implement-in-pytorch/blob/master/Classification/VGGnet(2014).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-obb3MkxiS0",
        "outputId": "bd124aec-6249-4e85-dc8a-762ff90c7d2b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('vggnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at vggnet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M2bOS5W62SA"
      },
      "source": [
        "# 1. 데이터셋 불러오기\r\n",
        "\r\n",
        "데이터셋은 torchvision 패키지에서 제공하는 STL10 dataset을 이용하겠습니다.\r\n",
        "\r\n",
        "STL10 dataset은 10개의 label을 갖습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFH8bsYnCixL"
      },
      "source": [
        "# import package\r\n",
        "\r\n",
        "# model\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchsummary import summary\r\n",
        "from torch import optim\r\n",
        "from torch.optim.lr_scheduler import StepLR\r\n",
        "\r\n",
        "# dataset and transformation\r\n",
        "from torchvision import datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import os\r\n",
        "\r\n",
        "# display images\r\n",
        "from torchvision import utils\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# utils\r\n",
        "import numpy as np\r\n",
        "from torchsummary import summary\r\n",
        "import time\r\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1aBOt5T1Irb",
        "outputId": "094eaa10-201b-42a5-915a-a4b8b5333251"
      },
      "source": [
        "# specify a data path\r\n",
        "path2data = '/content/vggnet/MyDrive/data'\r\n",
        "\r\n",
        "# if not exists the path, make the directory\r\n",
        "if not os.path.exists(path2data):\r\n",
        "    os.mkdir(path2data)\r\n",
        "\r\n",
        "# load dataset\r\n",
        "train_ds = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\r\n",
        "val_ds = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXFhAO-u_gru",
        "outputId": "88b727aa-a516-4f57-9f92-b2f9a32be63a"
      },
      "source": [
        "# check train_ds\r\n",
        "img, _ = train_ds[1]\r\n",
        "print(img.shape)\r\n",
        "\r\n",
        "print(len(train_ds))\r\n",
        "print(len(val_ds))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 96, 96])\n",
            "5000\n",
            "8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmfNBzPhyYCP",
        "outputId": "683107d6-1c7b-477d-e4af-c613d711cb89"
      },
      "source": [
        "# To normalize the dataset, calculate the mean and std\r\n",
        "train_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in train_ds]\r\n",
        "train_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in train_ds]\r\n",
        "\r\n",
        "train_meanR = np.mean([m[0] for m in train_meanRGB])\r\n",
        "train_meanG = np.mean([m[1] for m in train_meanRGB])\r\n",
        "train_meanB = np.mean([m[2] for m in train_meanRGB])\r\n",
        "train_stdR = np.mean([s[0] for s in train_stdRGB])\r\n",
        "train_stdG = np.mean([s[1] for s in train_stdRGB])\r\n",
        "train_stdB = np.mean([s[2] for s in train_stdRGB])\r\n",
        "\r\n",
        "\r\n",
        "val_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in val_ds]\r\n",
        "val_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in val_ds]\r\n",
        "\r\n",
        "val_meanR = np.mean([m[0] for m in val_meanRGB])\r\n",
        "val_meanG = np.mean([m[1] for m in val_meanRGB])\r\n",
        "val_meanB = np.mean([m[2] for m in val_meanRGB])\r\n",
        "\r\n",
        "val_stdR = np.mean([s[0] for s in val_stdRGB])\r\n",
        "val_stdG = np.mean([s[1] for s in val_stdRGB])\r\n",
        "val_stdB = np.mean([s[2] for s in val_stdRGB])\r\n",
        "\r\n",
        "print(train_meanR, train_meanG, train_meanB)\r\n",
        "print(val_meanR, val_meanG, val_meanB)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4467106 0.43980986 0.40664646\n",
            "0.44723064 0.4396425 0.40495726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51LcVkzd_zGr"
      },
      "source": [
        "# define the image transformation\r\n",
        "# using FiveCrop, normalize, horizontal reflection\r\n",
        "train_transformer = transforms.Compose([\r\n",
        "                    transforms.ToTensor(),\r\n",
        "                    transforms.Resize(224),\r\n",
        "                    transforms.Normalize([train_meanR, train_meanG, train_meanB], [train_stdR, train_stdG, train_stdB]),\r\n",
        "])\r\n",
        "\r\n",
        "# test_transformer = transforms.Compose([\r\n",
        "#                     transforms.ToTensor(),\r\n",
        "#                     transforms.Resize(224),\r\n",
        "#                     transforms.Normalize([train_meanR, train_meanG, train_meanB], [train_stdR, train_stdG, train_stdB]),\r\n",
        "# ])\r\n",
        "\r\n",
        "# apply transformation\r\n",
        "train_ds.transform = train_transformer\r\n",
        "val_ds.transform = train_transformer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvytEy0SCYqI"
      },
      "source": [
        "# display transformed sample images\r\n",
        "\r\n",
        "def show(imgs, y=None, color=True):\r\n",
        "    # for i, img in enumerate(imgs):\r\n",
        "    #     npimg = img.numpy()\r\n",
        "    #     npimg_tr = np.transpose(npimg, (1, 2, 0))\r\n",
        "    #     plt.subplot(1, imgs.shape[0], i+1)\r\n",
        "    #     plt.imshow(npimg_tr)\r\n",
        "\r\n",
        "    npimg = imgs.numpy()\r\n",
        "    npimg_tr = np.transpose(npimg, (1, 2, 0))\r\n",
        "    plt.imshow(npimg_tr)\r\n",
        "    \r\n",
        "    # plt.imshow(npimg_tr)\r\n",
        "    if y is not None:\r\n",
        "        plt.title('labels: ' + str(y))\r\n",
        "\r\n",
        "np.random.seed(0)\r\n",
        "torch.manual_seed(0)\r\n",
        "\r\n",
        "# pick a random sample image\r\n",
        "rnd_inds = int(np.random.randint(0, len(train_ds), 1))\r\n",
        "img, label = train_ds[rnd_inds]\r\n",
        "print('images indices: ', rnd_inds)\r\n",
        "\r\n",
        "plt.figure(figsize=(20, 20))\r\n",
        "show(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXmmPuFyDgLX"
      },
      "source": [
        "# create dataloader\r\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\r\n",
        "val_dl = DataLoader(val_ds, batch_size=32, shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKO1CzhyFK5D"
      },
      "source": [
        "# 2. 모델 구축하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ZSw0-xMojZ"
      },
      "source": [
        "# VGG type dict\r\n",
        "# int : output chnnels after conv layer\r\n",
        "# 'M' : max pooling layer\r\n",
        "VGG_types = {\r\n",
        "    'VGG11' : [64, 'M', 128, 'M', 256, 256, 'M', 512,512, 'M',512,512,'M'],\r\n",
        "    'VGG13' : [64,64, 'M', 128, 128, 'M', 256, 256, 'M', 512,512, 'M', 512,512,'M'],\r\n",
        "    'VGG16' : [64,64, 'M', 128, 128, 'M', 256, 256,256, 'M', 512,512,512, 'M',512,512,512,'M'],\r\n",
        "    'VGG19' : [64,64, 'M', 128, 128, 'M', 256, 256,256,256, 'M', 512,512,512,512, 'M',512,512,512,512,'M']\r\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFjyyOsLFF0B",
        "outputId": "0d64f129-d44b-4a04-c07a-9129ad69c435"
      },
      "source": [
        "# define VGGnet class\r\n",
        "class VGGnet(nn.Module):\r\n",
        "    def __init__(self, model, in_channels=3, num_classes=10, init_weights=True):\r\n",
        "        super(VGGnet,self).__init__()\r\n",
        "        self.in_channels = in_channels\r\n",
        "\r\n",
        "        # create conv_layers corresponding to VGG type\r\n",
        "        self.conv_layers = self.create_conv_laters(VGG_types[model])\r\n",
        "\r\n",
        "        self.fcs = nn.Sequential(\r\n",
        "            nn.Linear(512 * 7 * 7, 4096),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Dropout(),\r\n",
        "            nn.Linear(4096, 4096),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Dropout(),\r\n",
        "            nn.Linear(4096, num_classes),\r\n",
        "        )\r\n",
        "\r\n",
        "        # weight initialization\r\n",
        "        if init_weights:\r\n",
        "            self._initialize_weights()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv_layers(x)\r\n",
        "        x = x.view(-1, 512 * 7 * 7)\r\n",
        "        x = self.fcs(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    # define weight initialization function\r\n",
        "    def _initialize_weights(self):\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
        "                if m.bias is not None:\r\n",
        "                    nn.init.constant_(m.bias, 0)\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "    \r\n",
        "    # define a function to create conv layer taken the key of VGG_type dict \r\n",
        "    def create_conv_laters(self, architecture):\r\n",
        "        layers = []\r\n",
        "        in_channels = self.in_channels # 3\r\n",
        "\r\n",
        "        for x in architecture:\r\n",
        "            if type(x) == int: # int means conv layer\r\n",
        "                out_channels = x\r\n",
        "\r\n",
        "                layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\r\n",
        "                                     kernel_size=(3,3), stride=(1,1), padding=(1,1)),\r\n",
        "                           nn.BatchNorm2d(x),\r\n",
        "                           nn.ReLU()]\r\n",
        "                in_channels = x\r\n",
        "            elif x == 'M':\r\n",
        "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\r\n",
        "        \r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "# define device\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)\r\n",
        "\r\n",
        "# creat VGGnet object\r\n",
        "model = VGGnet('VGG16', in_channels=3, num_classes=10, init_weights=True).to(device)\r\n",
        "print(model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "VGGnet(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU()\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU()\n",
            "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU()\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU()\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU()\n",
            "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU()\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU()\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU()\n",
            "    (33): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU()\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU()\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU()\n",
            "    (43): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fcs): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNTvl3R0Pw2r",
        "outputId": "729b7d33-7079-48bb-af74-a62e59aa07b6"
      },
      "source": [
        "# print model summary\r\n",
        "summary(model, input_size=(3, 224, 224), device=device.type)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
            "              ReLU-3         [-1, 64, 224, 224]               0\n",
            "            Conv2d-4         [-1, 64, 224, 224]          36,928\n",
            "       BatchNorm2d-5         [-1, 64, 224, 224]             128\n",
            "              ReLU-6         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-7         [-1, 64, 112, 112]               0\n",
            "            Conv2d-8        [-1, 128, 112, 112]          73,856\n",
            "       BatchNorm2d-9        [-1, 128, 112, 112]             256\n",
            "             ReLU-10        [-1, 128, 112, 112]               0\n",
            "           Conv2d-11        [-1, 128, 112, 112]         147,584\n",
            "      BatchNorm2d-12        [-1, 128, 112, 112]             256\n",
            "             ReLU-13        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-14          [-1, 128, 56, 56]               0\n",
            "           Conv2d-15          [-1, 256, 56, 56]         295,168\n",
            "      BatchNorm2d-16          [-1, 256, 56, 56]             512\n",
            "             ReLU-17          [-1, 256, 56, 56]               0\n",
            "           Conv2d-18          [-1, 256, 56, 56]         590,080\n",
            "      BatchNorm2d-19          [-1, 256, 56, 56]             512\n",
            "             ReLU-20          [-1, 256, 56, 56]               0\n",
            "           Conv2d-21          [-1, 256, 56, 56]         590,080\n",
            "      BatchNorm2d-22          [-1, 256, 56, 56]             512\n",
            "             ReLU-23          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-24          [-1, 256, 28, 28]               0\n",
            "           Conv2d-25          [-1, 512, 28, 28]       1,180,160\n",
            "      BatchNorm2d-26          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-27          [-1, 512, 28, 28]               0\n",
            "           Conv2d-28          [-1, 512, 28, 28]       2,359,808\n",
            "      BatchNorm2d-29          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-30          [-1, 512, 28, 28]               0\n",
            "           Conv2d-31          [-1, 512, 28, 28]       2,359,808\n",
            "      BatchNorm2d-32          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-33          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-34          [-1, 512, 14, 14]               0\n",
            "           Conv2d-35          [-1, 512, 14, 14]       2,359,808\n",
            "      BatchNorm2d-36          [-1, 512, 14, 14]           1,024\n",
            "             ReLU-37          [-1, 512, 14, 14]               0\n",
            "           Conv2d-38          [-1, 512, 14, 14]       2,359,808\n",
            "      BatchNorm2d-39          [-1, 512, 14, 14]           1,024\n",
            "             ReLU-40          [-1, 512, 14, 14]               0\n",
            "           Conv2d-41          [-1, 512, 14, 14]       2,359,808\n",
            "      BatchNorm2d-42          [-1, 512, 14, 14]           1,024\n",
            "             ReLU-43          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-44            [-1, 512, 7, 7]               0\n",
            "           Linear-45                 [-1, 4096]     102,764,544\n",
            "             ReLU-46                 [-1, 4096]               0\n",
            "          Dropout-47                 [-1, 4096]               0\n",
            "           Linear-48                 [-1, 4096]      16,781,312\n",
            "             ReLU-49                 [-1, 4096]               0\n",
            "          Dropout-50                 [-1, 4096]               0\n",
            "           Linear-51                   [-1, 10]          40,970\n",
            "================================================================\n",
            "Total params: 134,309,962\n",
            "Trainable params: 134,309,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 321.94\n",
            "Params size (MB): 512.35\n",
            "Estimated Total Size (MB): 834.87\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFj6geDUTPkl"
      },
      "source": [
        "# 3. 모델 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ_CXj23St6c",
        "outputId": "e6464a67-666c-4e83-f656-083b46d221ac"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss(reduction=\"sum\")\r\n",
        "opt = optim.Adam(model.parameters(), lr=0.001)\r\n",
        "\r\n",
        "# get learning rate \r\n",
        "def get_lr(opt):\r\n",
        "    for param_group in opt.param_groups:\r\n",
        "        return param_group['lr']\r\n",
        "\r\n",
        "current_lr = get_lr(opt)\r\n",
        "print('current lr={}'.format(current_lr))\r\n",
        "\r\n",
        "# define learning rate scheduler\r\n",
        "# from torch.optim.lr_scheduler import CosineAnnealingLR\r\n",
        "# lr_scheduler = CosineAnnealingLR(opt, T_max=2, eta_min=1e-5)\r\n",
        "\r\n",
        "from torch.optim.lr_scheduler import StepLR\r\n",
        "lr_scheduler = StepLR(opt, step_size=30, gamma=0.1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current lr=0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSLgEvTHWGs-"
      },
      "source": [
        "def metrics_batch(output, target):\r\n",
        "    # get output class\r\n",
        "    pred = output.argmax(dim=1, keepdim=True)\r\n",
        "    \r\n",
        "    # compare output class with target class\r\n",
        "    corrects=pred.eq(target.view_as(pred)).sum().item()\r\n",
        "    return corrects\r\n",
        "\r\n",
        "def loss_batch(loss_func, output, target, opt=None):\r\n",
        "    \r\n",
        "    # get loss \r\n",
        "    loss = loss_func(output, target)\r\n",
        "    \r\n",
        "    # get performance metric\r\n",
        "    metric_b = metrics_batch(output,target)\r\n",
        "    \r\n",
        "    if opt is not None:\r\n",
        "        opt.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        opt.step()\r\n",
        "\r\n",
        "    return loss.item(), metric_b\r\n",
        "\r\n",
        "def loss_epoch(model,loss_func,dataset_dl,sanity_check=False,opt=None):\r\n",
        "    running_loss=0.0\r\n",
        "    running_metric=0.0\r\n",
        "    len_data=len(dataset_dl.dataset)\r\n",
        "\r\n",
        "    for xb, yb in dataset_dl:\r\n",
        "        # move batch to device\r\n",
        "        xb=xb.to(device)\r\n",
        "        yb=yb.to(device)\r\n",
        "        output = model(xb)\r\n",
        "        # Five crop : bs, crops, chnnel, h, w\r\n",
        "        # making dimmension (bs, c, h, w)\r\n",
        "        # bs, ncrops, c, h, w = xb.size()\r\n",
        "        # output_=model(xb.view(-1, c, h, w))\r\n",
        "        # output = output_.view(bs, ncrops, -1).mean(1)\r\n",
        "        \r\n",
        "        # get loss per batch\r\n",
        "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt)\r\n",
        "        \r\n",
        "        # update running loss\r\n",
        "        running_loss+=loss_b\r\n",
        "        \r\n",
        "        # update running metric\r\n",
        "        if metric_b is not None:\r\n",
        "            running_metric+=metric_b\r\n",
        "\r\n",
        "        # break the loop in case of sanity check\r\n",
        "        if sanity_check is True:\r\n",
        "            break\r\n",
        "    \r\n",
        "    # average loss value\r\n",
        "    loss=running_loss/float(len_data)\r\n",
        "    \r\n",
        "    # average metric value\r\n",
        "    metric=running_metric/float(len_data)\r\n",
        "    \r\n",
        "    return loss, metric"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7U-Zo7fXPil"
      },
      "source": [
        "def train_val(model, params):\r\n",
        "    # extract model parameters\r\n",
        "    num_epochs=params[\"num_epochs\"]\r\n",
        "    loss_func=params[\"loss_func\"]\r\n",
        "    opt=params[\"optimizer\"]\r\n",
        "    train_dl=params[\"train_dl\"]\r\n",
        "    val_dl=params[\"val_dl\"]\r\n",
        "    sanity_check=params[\"sanity_check\"]\r\n",
        "    lr_scheduler=params[\"lr_scheduler\"]\r\n",
        "    path2weights=params[\"path2weights\"]\r\n",
        "    \r\n",
        "    # history of loss values in each epoch\r\n",
        "    loss_history={\r\n",
        "        \"train\": [],\r\n",
        "        \"val\": [],\r\n",
        "    }\r\n",
        "    \r\n",
        "    # histroy of metric values in each epoch\r\n",
        "    metric_history={\r\n",
        "        \"train\": [],\r\n",
        "        \"val\": [],\r\n",
        "    }\r\n",
        "    \r\n",
        "    # 가중치를 저장할 때, 코랩 GPU 오류나서 생략했습니다.\r\n",
        "    # a deep copy of weights for the best performing model\r\n",
        "    # best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "    \r\n",
        "    # initialize best loss to a large value\r\n",
        "    best_loss=float('inf')\r\n",
        "    \r\n",
        "    # check start time\r\n",
        "    start_time = time.time()\r\n",
        "    # main loop\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        # get current learning rate\r\n",
        "        current_lr=get_lr(opt)\r\n",
        "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))\r\n",
        "        \r\n",
        "        # train model on training dataset\r\n",
        "        model.train()\r\n",
        "        train_loss, train_metric=loss_epoch(model,loss_func,train_dl,sanity_check,opt)\r\n",
        "\r\n",
        "        # collect loss and metric for training dataset\r\n",
        "        loss_history[\"train\"].append(train_loss)\r\n",
        "        metric_history[\"train\"].append(train_metric)\r\n",
        "        \r\n",
        "        # evaluate model on validation dataset    \r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            val_loss, val_metric=loss_epoch(model,loss_func,val_dl,sanity_check)\r\n",
        "        \r\n",
        "       \r\n",
        "        # store best model\r\n",
        "        if val_loss < best_loss:\r\n",
        "            best_loss = val_loss\r\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "            \r\n",
        "            # # store weights into a local file\r\n",
        "            # torch.save(model.state_dict(), path2weights)\r\n",
        "            # print(\"Copied best model weights!\")\r\n",
        "        \r\n",
        "        # collect loss and metric for validation dataset\r\n",
        "        loss_history[\"val\"].append(val_loss)\r\n",
        "        metric_history[\"val\"].append(val_metric)\r\n",
        "        \r\n",
        "        # learning rate schedule\r\n",
        "        lr_scheduler.step()\r\n",
        "\r\n",
        "        print(\"train loss: %.6f, dev loss: %.6f, accuracy: %.2f, time: %.4f s\" %(train_loss,val_loss,100*val_metric, time.time()-start_time))\r\n",
        "        print(\"-\"*10) \r\n",
        "\r\n",
        "    ## load best model weights\r\n",
        "    # model.load_state_dict(best_model_wts)\r\n",
        "        \r\n",
        "    return model, loss_history, metric_history"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz10guodYFYV"
      },
      "source": [
        "# definc the training parameters\r\n",
        "params_train = {\r\n",
        "    'num_epochs':100,\r\n",
        "    'optimizer':opt,\r\n",
        "    'loss_func':loss_func,\r\n",
        "    'train_dl':train_dl,\r\n",
        "    'val_dl':val_dl,\r\n",
        "    'sanity_check':False,\r\n",
        "    'lr_scheduler':lr_scheduler,\r\n",
        "    'path2weights':'./models/weights.pt',\r\n",
        "}\r\n",
        "\r\n",
        "# create the directory that stores weights.pt\r\n",
        "def createFolder(directory):\r\n",
        "    try:\r\n",
        "        if not os.path.exists(directory):\r\n",
        "            os.makedirs(directory)\r\n",
        "    except OSerror:\r\n",
        "        print('Error')\r\n",
        "createFolder('./models')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKosxtFGYMV7",
        "outputId": "cff4541c-8f94-46d6-a9d0-62b457f15adf"
      },
      "source": [
        "# train model\r\n",
        "model, loss_hist, metric_hist = train_val(model, params_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99, current lr=0.001\n",
            "train loss: 4.258943, dev loss: 2.121509, accuracy: 18.44, time: 280.3598 s\n",
            "----------\n",
            "Epoch 1/99, current lr=0.001\n",
            "train loss: 2.164895, dev loss: 2.070191, accuracy: 22.46, time: 558.0739 s\n",
            "----------\n",
            "Epoch 2/99, current lr=0.001\n",
            "train loss: 2.128880, dev loss: 2.016709, accuracy: 23.41, time: 834.8796 s\n",
            "----------\n",
            "Epoch 3/99, current lr=0.001\n",
            "train loss: 2.131476, dev loss: 2.029797, accuracy: 23.61, time: 1114.5844 s\n",
            "----------\n",
            "Epoch 4/99, current lr=0.001\n",
            "train loss: 2.090199, dev loss: 2.054837, accuracy: 18.81, time: 1393.1732 s\n",
            "----------\n",
            "Epoch 5/99, current lr=0.001\n",
            "train loss: 2.087067, dev loss: 1.982239, accuracy: 23.66, time: 1673.5301 s\n",
            "----------\n",
            "Epoch 6/99, current lr=0.001\n",
            "train loss: 2.056646, dev loss: 2.009145, accuracy: 23.97, time: 1955.3066 s\n",
            "----------\n",
            "Epoch 7/99, current lr=0.001\n",
            "train loss: 2.011112, dev loss: 1.959518, accuracy: 22.66, time: 2235.1641 s\n",
            "----------\n",
            "Epoch 8/99, current lr=0.001\n",
            "train loss: 1.975343, dev loss: 1.860020, accuracy: 27.47, time: 2516.7926 s\n",
            "----------\n",
            "Epoch 9/99, current lr=0.001\n",
            "train loss: 1.960512, dev loss: 1.917755, accuracy: 26.00, time: 2798.7557 s\n",
            "----------\n",
            "Epoch 10/99, current lr=0.001\n",
            "train loss: 1.851995, dev loss: 1.813446, accuracy: 27.38, time: 3078.6891 s\n",
            "----------\n",
            "Epoch 11/99, current lr=0.001\n",
            "train loss: 1.830508, dev loss: 1.744496, accuracy: 29.81, time: 3360.5078 s\n",
            "----------\n",
            "Epoch 12/99, current lr=0.001\n",
            "train loss: 1.807125, dev loss: 1.726220, accuracy: 29.95, time: 3643.0027 s\n",
            "----------\n",
            "Epoch 13/99, current lr=0.001\n",
            "train loss: 1.758253, dev loss: 1.737879, accuracy: 31.30, time: 3923.1503 s\n",
            "----------\n",
            "Epoch 14/99, current lr=0.001\n",
            "train loss: 1.736055, dev loss: 1.692480, accuracy: 31.29, time: 4204.9407 s\n",
            "----------\n",
            "Epoch 15/99, current lr=0.001\n",
            "train loss: 1.759827, dev loss: 1.654770, accuracy: 32.25, time: 4486.9833 s\n",
            "----------\n",
            "Epoch 16/99, current lr=0.001\n",
            "train loss: 1.712420, dev loss: 1.604183, accuracy: 35.74, time: 4766.9571 s\n",
            "----------\n",
            "Epoch 17/99, current lr=0.001\n",
            "train loss: 1.650242, dev loss: 1.602285, accuracy: 35.50, time: 5046.5600 s\n",
            "----------\n",
            "Epoch 18/99, current lr=0.001\n",
            "train loss: 1.609045, dev loss: 1.564369, accuracy: 37.95, time: 5328.1097 s\n",
            "----------\n",
            "Epoch 19/99, current lr=0.001\n",
            "train loss: 1.565167, dev loss: 1.599424, accuracy: 36.91, time: 5610.1728 s\n",
            "----------\n",
            "Epoch 20/99, current lr=0.001\n",
            "train loss: 1.588204, dev loss: 1.674823, accuracy: 38.20, time: 5890.1501 s\n",
            "----------\n",
            "Epoch 21/99, current lr=0.001\n",
            "train loss: 1.594831, dev loss: 1.686040, accuracy: 34.70, time: 6172.3685 s\n",
            "----------\n",
            "Epoch 22/99, current lr=0.001\n",
            "train loss: 1.543201, dev loss: 1.475946, accuracy: 43.38, time: 6452.0897 s\n",
            "----------\n",
            "Epoch 23/99, current lr=0.001\n",
            "train loss: 1.480673, dev loss: 1.450636, accuracy: 42.75, time: 6733.8198 s\n",
            "----------\n",
            "Epoch 24/99, current lr=0.001\n",
            "train loss: 1.442022, dev loss: 1.465584, accuracy: 44.64, time: 7015.9144 s\n",
            "----------\n",
            "Epoch 25/99, current lr=0.001\n",
            "train loss: 1.409526, dev loss: 1.378128, accuracy: 47.44, time: 7295.6884 s\n",
            "----------\n",
            "Epoch 26/99, current lr=0.001\n",
            "train loss: 1.351372, dev loss: 1.294753, accuracy: 52.85, time: 7577.5172 s\n",
            "----------\n",
            "Epoch 27/99, current lr=0.001\n",
            "train loss: 1.301974, dev loss: 1.274938, accuracy: 53.70, time: 7859.6030 s\n",
            "----------\n",
            "Epoch 28/99, current lr=0.001\n",
            "train loss: 1.250775, dev loss: 1.288680, accuracy: 51.84, time: 8139.6596 s\n",
            "----------\n",
            "Epoch 29/99, current lr=0.001\n",
            "train loss: 1.195772, dev loss: 1.245904, accuracy: 54.09, time: 8421.3386 s\n",
            "----------\n",
            "Epoch 30/99, current lr=0.0001\n",
            "train loss: 1.034900, dev loss: 1.141978, accuracy: 58.09, time: 8703.6442 s\n",
            "----------\n",
            "Epoch 31/99, current lr=0.0001\n",
            "train loss: 0.982705, dev loss: 1.120397, accuracy: 58.86, time: 8983.9506 s\n",
            "----------\n",
            "Epoch 32/99, current lr=0.0001\n",
            "train loss: 0.941440, dev loss: 1.116262, accuracy: 59.36, time: 9263.3276 s\n",
            "----------\n",
            "Epoch 33/99, current lr=0.0001\n",
            "train loss: 0.920915, dev loss: 1.109552, accuracy: 59.77, time: 9544.6740 s\n",
            "----------\n",
            "Epoch 34/99, current lr=0.0001\n",
            "train loss: 0.901967, dev loss: 1.105480, accuracy: 60.27, time: 9826.3323 s\n",
            "----------\n",
            "Epoch 35/99, current lr=0.0001\n",
            "train loss: 0.881843, dev loss: 1.094380, accuracy: 59.81, time: 10105.9526 s\n",
            "----------\n",
            "Epoch 36/99, current lr=0.0001\n",
            "train loss: 0.856297, dev loss: 1.098286, accuracy: 60.85, time: 10385.2806 s\n",
            "----------\n",
            "Epoch 37/99, current lr=0.0001\n",
            "train loss: 0.840634, dev loss: 1.100958, accuracy: 60.82, time: 10666.9544 s\n",
            "----------\n",
            "Epoch 38/99, current lr=0.0001\n",
            "train loss: 0.822127, dev loss: 1.103165, accuracy: 60.66, time: 10946.4056 s\n",
            "----------\n",
            "Epoch 39/99, current lr=0.0001\n",
            "train loss: 0.797038, dev loss: 1.096758, accuracy: 60.80, time: 11228.4888 s\n",
            "----------\n",
            "Epoch 40/99, current lr=0.0001\n",
            "train loss: 0.790094, dev loss: 1.110498, accuracy: 60.70, time: 11508.5342 s\n",
            "----------\n",
            "Epoch 41/99, current lr=0.0001\n",
            "train loss: 0.770408, dev loss: 1.109027, accuracy: 60.77, time: 11790.6655 s\n",
            "----------\n",
            "Epoch 42/99, current lr=0.0001\n",
            "train loss: 0.748125, dev loss: 1.136262, accuracy: 61.22, time: 12070.7732 s\n",
            "----------\n",
            "Epoch 43/99, current lr=0.0001\n",
            "train loss: 0.727048, dev loss: 1.090439, accuracy: 61.74, time: 12353.0831 s\n",
            "----------\n",
            "Epoch 44/99, current lr=0.0001\n",
            "train loss: 0.694027, dev loss: 1.081915, accuracy: 62.40, time: 12633.3339 s\n",
            "----------\n",
            "Epoch 45/99, current lr=0.0001\n",
            "train loss: 0.682349, dev loss: 1.128999, accuracy: 62.29, time: 12913.4931 s\n",
            "----------\n",
            "Epoch 46/99, current lr=0.0001\n",
            "train loss: 0.662263, dev loss: 1.120047, accuracy: 62.28, time: 13195.4859 s\n",
            "----------\n",
            "Epoch 47/99, current lr=0.0001\n",
            "train loss: 0.638412, dev loss: 1.141926, accuracy: 62.42, time: 13475.7896 s\n",
            "----------\n",
            "Epoch 48/99, current lr=0.0001\n",
            "train loss: 0.624713, dev loss: 1.142247, accuracy: 62.71, time: 13758.1088 s\n",
            "----------\n",
            "Epoch 49/99, current lr=0.0001\n",
            "train loss: 0.617208, dev loss: 1.136010, accuracy: 62.88, time: 14038.3655 s\n",
            "----------\n",
            "Epoch 50/99, current lr=0.0001\n",
            "train loss: 0.571359, dev loss: 1.135339, accuracy: 62.92, time: 14320.6652 s\n",
            "----------\n",
            "Epoch 51/99, current lr=0.0001\n",
            "train loss: 0.555517, dev loss: 1.150543, accuracy: 62.54, time: 14600.8480 s\n",
            "----------\n",
            "Epoch 52/99, current lr=0.0001\n",
            "train loss: 0.546870, dev loss: 1.180299, accuracy: 62.19, time: 14883.0082 s\n",
            "----------\n",
            "Epoch 53/99, current lr=0.0001\n",
            "train loss: 0.506459, dev loss: 1.188590, accuracy: 63.20, time: 15163.2208 s\n",
            "----------\n",
            "Epoch 54/99, current lr=0.0001\n",
            "train loss: 0.504179, dev loss: 1.191714, accuracy: 62.98, time: 15445.2608 s\n",
            "----------\n",
            "Epoch 55/99, current lr=0.0001\n",
            "train loss: 0.475946, dev loss: 1.226681, accuracy: 63.39, time: 15725.2849 s\n",
            "----------\n",
            "Epoch 56/99, current lr=0.0001\n",
            "train loss: 0.456591, dev loss: 1.242396, accuracy: 63.28, time: 16007.3702 s\n",
            "----------\n",
            "Epoch 57/99, current lr=0.0001\n",
            "train loss: 0.440009, dev loss: 1.196428, accuracy: 63.46, time: 16287.4824 s\n",
            "----------\n",
            "Epoch 58/99, current lr=0.0001\n",
            "train loss: 0.433623, dev loss: 1.296875, accuracy: 63.59, time: 16569.6686 s\n",
            "----------\n",
            "Epoch 59/99, current lr=0.0001\n",
            "train loss: 0.429161, dev loss: 1.245340, accuracy: 63.79, time: 16849.8635 s\n",
            "----------\n",
            "Epoch 60/99, current lr=1e-05\n",
            "train loss: 0.383100, dev loss: 1.252378, accuracy: 63.78, time: 17131.8971 s\n",
            "----------\n",
            "Epoch 61/99, current lr=1e-05\n",
            "train loss: 0.377371, dev loss: 1.228582, accuracy: 64.39, time: 17411.9846 s\n",
            "----------\n",
            "Epoch 62/99, current lr=1e-05\n",
            "train loss: 0.376989, dev loss: 1.246094, accuracy: 64.35, time: 17694.0067 s\n",
            "----------\n",
            "Epoch 63/99, current lr=1e-05\n",
            "train loss: 0.356036, dev loss: 1.251844, accuracy: 64.04, time: 17974.1158 s\n",
            "----------\n",
            "Epoch 64/99, current lr=1e-05\n",
            "train loss: 0.362810, dev loss: 1.260071, accuracy: 64.64, time: 18256.0583 s\n",
            "----------\n",
            "Epoch 65/99, current lr=1e-05\n",
            "train loss: 0.348526, dev loss: 1.265681, accuracy: 64.89, time: 18535.9339 s\n",
            "----------\n",
            "Epoch 66/99, current lr=1e-05\n",
            "train loss: 0.350941, dev loss: 1.250109, accuracy: 64.33, time: 18817.9225 s\n",
            "----------\n",
            "Epoch 67/99, current lr=1e-05\n",
            "train loss: 0.349113, dev loss: 1.252525, accuracy: 64.45, time: 19098.2829 s\n",
            "----------\n",
            "Epoch 68/99, current lr=1e-05\n",
            "train loss: 0.329761, dev loss: 1.312148, accuracy: 64.33, time: 19380.4348 s\n",
            "----------\n",
            "Epoch 69/99, current lr=1e-05\n",
            "train loss: 0.340817, dev loss: 1.261608, accuracy: 64.68, time: 19660.3867 s\n",
            "----------\n",
            "Epoch 70/99, current lr=1e-05\n",
            "train loss: 0.338766, dev loss: 1.303153, accuracy: 64.08, time: 19942.4154 s\n",
            "----------\n",
            "Epoch 71/99, current lr=1e-05\n",
            "train loss: 0.337513, dev loss: 1.307978, accuracy: 63.99, time: 20222.2924 s\n",
            "----------\n",
            "Epoch 72/99, current lr=1e-05\n",
            "train loss: 0.332154, dev loss: 1.291724, accuracy: 64.26, time: 20504.1157 s\n",
            "----------\n",
            "Epoch 73/99, current lr=1e-05\n",
            "train loss: 0.349033, dev loss: 1.285028, accuracy: 64.08, time: 20784.3046 s\n",
            "----------\n",
            "Epoch 74/99, current lr=1e-05\n",
            "train loss: 0.337804, dev loss: 1.280288, accuracy: 64.19, time: 21066.3321 s\n",
            "----------\n",
            "Epoch 75/99, current lr=1e-05\n",
            "train loss: 0.328998, dev loss: 1.309848, accuracy: 64.25, time: 21346.3242 s\n",
            "----------\n",
            "Epoch 76/99, current lr=1e-05\n",
            "train loss: 0.329356, dev loss: 1.283551, accuracy: 64.45, time: 21628.3988 s\n",
            "----------\n",
            "Epoch 77/99, current lr=1e-05\n",
            "train loss: 0.327972, dev loss: 1.307014, accuracy: 64.40, time: 21908.3355 s\n",
            "----------\n",
            "Epoch 78/99, current lr=1e-05\n",
            "train loss: 0.330954, dev loss: 1.289278, accuracy: 64.20, time: 22190.4419 s\n",
            "----------\n",
            "Epoch 79/99, current lr=1e-05\n",
            "train loss: 0.314474, dev loss: 1.309071, accuracy: 64.31, time: 22470.5109 s\n",
            "----------\n",
            "Epoch 80/99, current lr=1e-05\n",
            "train loss: 0.324875, dev loss: 1.294324, accuracy: 64.50, time: 22752.7132 s\n",
            "----------\n",
            "Epoch 81/99, current lr=1e-05\n",
            "train loss: 0.316102, dev loss: 1.362930, accuracy: 64.00, time: 23032.3571 s\n",
            "----------\n",
            "Epoch 82/99, current lr=1e-05\n",
            "train loss: 0.316274, dev loss: 1.323906, accuracy: 64.59, time: 23313.0950 s\n",
            "----------\n",
            "Epoch 83/99, current lr=1e-05\n",
            "train loss: 0.321087, dev loss: 1.316016, accuracy: 64.35, time: 23593.2219 s\n",
            "----------\n",
            "Epoch 84/99, current lr=1e-05\n",
            "train loss: 0.315823, dev loss: 1.315134, accuracy: 64.56, time: 23874.1883 s\n",
            "----------\n",
            "Epoch 85/99, current lr=1e-05\n",
            "train loss: 0.302346, dev loss: 1.357225, accuracy: 64.48, time: 24154.6759 s\n",
            "----------\n",
            "Epoch 86/99, current lr=1e-05\n",
            "train loss: 0.296479, dev loss: 1.328814, accuracy: 64.61, time: 24437.1213 s\n",
            "----------\n",
            "Epoch 87/99, current lr=1e-05\n",
            "train loss: 0.322255, dev loss: 1.343270, accuracy: 64.14, time: 24717.4760 s\n",
            "----------\n",
            "Epoch 88/99, current lr=1e-05\n",
            "train loss: 0.305131, dev loss: 1.342667, accuracy: 64.35, time: 24999.4636 s\n",
            "----------\n",
            "Epoch 89/99, current lr=1e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "867B4PrGYYq9"
      },
      "source": [
        "# Train-Validation Progress\r\n",
        "num_epochs=params_train[\"num_epochs\"]\r\n",
        "\r\n",
        "# plot loss progress\r\n",
        "plt.title(\"Train-Val Loss\")\r\n",
        "plt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\r\n",
        "plt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.xlabel(\"Training Epochs\")\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# plot accuracy progress\r\n",
        "plt.title(\"Train-Val Accuracy\")\r\n",
        "plt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\r\n",
        "plt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\r\n",
        "plt.ylabel(\"Accuracy\")\r\n",
        "plt.xlabel(\"Training Epochs\")\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XHUcXC7gJnN"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\r\n",
        "class_total = list(0. for i in range(10))\r\n",
        "with torch.no_grad():\r\n",
        "    for xb, yb in val_dl:\r\n",
        "        xb = xb.to(device)\r\n",
        "        yb = yb.to(device)\r\n",
        "        output = model(xb)\r\n",
        "        pred = output.argmax(dim=1, keepdim=True)\r\n",
        "        corrects = pred.eq(output.view_as(pred)).sum().item()\r\n",
        "        for i in range(4):\r\n",
        "            yb = yb[i]\r\n",
        "            class_correct[yb] += corrects[i].item()\r\n",
        "            class_total[yb] += 1\r\n",
        "\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    print('Accuracy of %5s : %2d %%' % (\r\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}