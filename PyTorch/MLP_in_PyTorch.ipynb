{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MLP(Multi-Layer Perceptron)\n",
        "----\n",
        "* 지도학습에 사용되는 인공 신경망의 한 형태\n",
        "* MLP는 일반적으로 최소 하나 이상의 비선형 은닉 계층을 포함하며, 이러한 계층은 학습 데이터에서 복잡한 패턴을 추출하는 데 도움이 됨\n",
        "* MLP는 주로 분류 및 회귀 문제에 적용되며, 그 학습 알고리즘으로 역전파가 주로 사용\n",
        "* 입력 계층, 하나 이상의 은닉 계층, 그리고 출력 계층으로 구성\n",
        "* 각 계층은 노드(또는 뉴런)로 구성되며, 이들은 다음 계층에 있는 노드로 완전히 연결되어 있음\n",
        "* MLP의 핵심 개념은 \"깊이\" 혹은 \"층\"에 있는데, 이는 복잡한 패턴을 학습하는 데 필요한 노드의 개수와 결합의 복잡성을 증가시킴\n",
        "* 아래의 학습 결과로 layer를 많이 쌓을수록 train loss의 값은 낮출 수 있음을 확인 가능 -> Overfitting의 가능성 존재\n"
      ],
      "metadata": {
        "id": "T43ryfWzmYLR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rt63f8XbTrgb"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "dataset = datasets.load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "X = torch.tensor(dataset.data, dtype=torch.float32) # numpy array를 tensor로 변경\n",
        "y = torch.tensor(dataset.target)"
      ],
      "metadata": {
        "id": "3o__mb9CUZoU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, hidden_units):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(4, hidden_units)\n",
        "    self.fc2 = nn.Linear(hidden_units, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "lsatWxd6T7vH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(5)"
      ],
      "metadata": {
        "id": "78oXC-F5LiqG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "TOZmBIh3UnT6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion):\n",
        "  for epoch in range(100):\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = criterion(y_pred, y)\n",
        "    print(f\"Epoch: {epoch} / Loss: {loss}\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "train(model, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTngoWE1Vsxi",
        "outputId": "9c97d7fe-ac8e-4fc1-a039-ea7aeb101da9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 / Loss: 1.202958583831787\n",
            "Epoch: 1 / Loss: 1.202958583831787\n",
            "Epoch: 2 / Loss: 1.202958583831787\n",
            "Epoch: 3 / Loss: 1.202958583831787\n",
            "Epoch: 4 / Loss: 1.202958583831787\n",
            "Epoch: 5 / Loss: 1.202958583831787\n",
            "Epoch: 6 / Loss: 1.202958583831787\n",
            "Epoch: 7 / Loss: 1.202958583831787\n",
            "Epoch: 8 / Loss: 1.202958583831787\n",
            "Epoch: 9 / Loss: 1.202958583831787\n",
            "Epoch: 10 / Loss: 1.202958583831787\n",
            "Epoch: 11 / Loss: 1.202958583831787\n",
            "Epoch: 12 / Loss: 1.202958583831787\n",
            "Epoch: 13 / Loss: 1.202958583831787\n",
            "Epoch: 14 / Loss: 1.202958583831787\n",
            "Epoch: 15 / Loss: 1.202958583831787\n",
            "Epoch: 16 / Loss: 1.202958583831787\n",
            "Epoch: 17 / Loss: 1.202958583831787\n",
            "Epoch: 18 / Loss: 1.202958583831787\n",
            "Epoch: 19 / Loss: 1.202958583831787\n",
            "Epoch: 20 / Loss: 1.202958583831787\n",
            "Epoch: 21 / Loss: 1.202958583831787\n",
            "Epoch: 22 / Loss: 1.202958583831787\n",
            "Epoch: 23 / Loss: 1.202958583831787\n",
            "Epoch: 24 / Loss: 1.202958583831787\n",
            "Epoch: 25 / Loss: 1.202958583831787\n",
            "Epoch: 26 / Loss: 1.202958583831787\n",
            "Epoch: 27 / Loss: 1.202958583831787\n",
            "Epoch: 28 / Loss: 1.202958583831787\n",
            "Epoch: 29 / Loss: 1.202958583831787\n",
            "Epoch: 30 / Loss: 1.202958583831787\n",
            "Epoch: 31 / Loss: 1.202958583831787\n",
            "Epoch: 32 / Loss: 1.202958583831787\n",
            "Epoch: 33 / Loss: 1.202958583831787\n",
            "Epoch: 34 / Loss: 1.202958583831787\n",
            "Epoch: 35 / Loss: 1.202958583831787\n",
            "Epoch: 36 / Loss: 1.202958583831787\n",
            "Epoch: 37 / Loss: 1.202958583831787\n",
            "Epoch: 38 / Loss: 1.202958583831787\n",
            "Epoch: 39 / Loss: 1.202958583831787\n",
            "Epoch: 40 / Loss: 1.202958583831787\n",
            "Epoch: 41 / Loss: 1.202958583831787\n",
            "Epoch: 42 / Loss: 1.202958583831787\n",
            "Epoch: 43 / Loss: 1.202958583831787\n",
            "Epoch: 44 / Loss: 1.202958583831787\n",
            "Epoch: 45 / Loss: 1.202958583831787\n",
            "Epoch: 46 / Loss: 1.202958583831787\n",
            "Epoch: 47 / Loss: 1.202958583831787\n",
            "Epoch: 48 / Loss: 1.202958583831787\n",
            "Epoch: 49 / Loss: 1.202958583831787\n",
            "Epoch: 50 / Loss: 1.202958583831787\n",
            "Epoch: 51 / Loss: 1.202958583831787\n",
            "Epoch: 52 / Loss: 1.202958583831787\n",
            "Epoch: 53 / Loss: 1.202958583831787\n",
            "Epoch: 54 / Loss: 1.202958583831787\n",
            "Epoch: 55 / Loss: 1.202958583831787\n",
            "Epoch: 56 / Loss: 1.202958583831787\n",
            "Epoch: 57 / Loss: 1.202958583831787\n",
            "Epoch: 58 / Loss: 1.202958583831787\n",
            "Epoch: 59 / Loss: 1.202958583831787\n",
            "Epoch: 60 / Loss: 1.202958583831787\n",
            "Epoch: 61 / Loss: 1.202958583831787\n",
            "Epoch: 62 / Loss: 1.202958583831787\n",
            "Epoch: 63 / Loss: 1.202958583831787\n",
            "Epoch: 64 / Loss: 1.202958583831787\n",
            "Epoch: 65 / Loss: 1.202958583831787\n",
            "Epoch: 66 / Loss: 1.202958583831787\n",
            "Epoch: 67 / Loss: 1.202958583831787\n",
            "Epoch: 68 / Loss: 1.202958583831787\n",
            "Epoch: 69 / Loss: 1.202958583831787\n",
            "Epoch: 70 / Loss: 1.202958583831787\n",
            "Epoch: 71 / Loss: 1.202958583831787\n",
            "Epoch: 72 / Loss: 1.202958583831787\n",
            "Epoch: 73 / Loss: 1.202958583831787\n",
            "Epoch: 74 / Loss: 1.202958583831787\n",
            "Epoch: 75 / Loss: 1.202958583831787\n",
            "Epoch: 76 / Loss: 1.202958583831787\n",
            "Epoch: 77 / Loss: 1.202958583831787\n",
            "Epoch: 78 / Loss: 1.202958583831787\n",
            "Epoch: 79 / Loss: 1.202958583831787\n",
            "Epoch: 80 / Loss: 1.202958583831787\n",
            "Epoch: 81 / Loss: 1.202958583831787\n",
            "Epoch: 82 / Loss: 1.202958583831787\n",
            "Epoch: 83 / Loss: 1.202958583831787\n",
            "Epoch: 84 / Loss: 1.202958583831787\n",
            "Epoch: 85 / Loss: 1.202958583831787\n",
            "Epoch: 86 / Loss: 1.202958583831787\n",
            "Epoch: 87 / Loss: 1.202958583831787\n",
            "Epoch: 88 / Loss: 1.202958583831787\n",
            "Epoch: 89 / Loss: 1.202958583831787\n",
            "Epoch: 90 / Loss: 1.202958583831787\n",
            "Epoch: 91 / Loss: 1.202958583831787\n",
            "Epoch: 92 / Loss: 1.202958583831787\n",
            "Epoch: 93 / Loss: 1.202958583831787\n",
            "Epoch: 94 / Loss: 1.202958583831787\n",
            "Epoch: 95 / Loss: 1.202958583831787\n",
            "Epoch: 96 / Loss: 1.202958583831787\n",
            "Epoch: 97 / Loss: 1.202958583831787\n",
            "Epoch: 98 / Loss: 1.202958583831787\n",
            "Epoch: 99 / Loss: 1.202958583831787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(100)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "train(model, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE7-CEbpV3pv",
        "outputId": "716caea8-6741-480f-8ff8-b88b55249c21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 / Loss: 1.3128803968429565\n",
            "Epoch: 1 / Loss: 3.8241353034973145\n",
            "Epoch: 2 / Loss: 8.122169494628906\n",
            "Epoch: 3 / Loss: 4.01040506362915\n",
            "Epoch: 4 / Loss: 1.5513596534729004\n",
            "Epoch: 5 / Loss: 0.9330595135688782\n",
            "Epoch: 6 / Loss: 0.8725282549858093\n",
            "Epoch: 7 / Loss: 0.600397527217865\n",
            "Epoch: 8 / Loss: 0.562197744846344\n",
            "Epoch: 9 / Loss: 0.5074576139450073\n",
            "Epoch: 10 / Loss: 0.4870047867298126\n",
            "Epoch: 11 / Loss: 0.4642579257488251\n",
            "Epoch: 12 / Loss: 0.45161697268486023\n",
            "Epoch: 13 / Loss: 0.4378647804260254\n",
            "Epoch: 14 / Loss: 0.4294222891330719\n",
            "Epoch: 15 / Loss: 0.4198106527328491\n",
            "Epoch: 16 / Loss: 0.41495341062545776\n",
            "Epoch: 17 / Loss: 0.4086577296257019\n",
            "Epoch: 18 / Loss: 0.40880775451660156\n",
            "Epoch: 19 / Loss: 0.40594419836997986\n",
            "Epoch: 20 / Loss: 0.4141470193862915\n",
            "Epoch: 21 / Loss: 0.41325750946998596\n",
            "Epoch: 22 / Loss: 0.4313598573207855\n",
            "Epoch: 23 / Loss: 0.42462968826293945\n",
            "Epoch: 24 / Loss: 0.4469865560531616\n",
            "Epoch: 25 / Loss: 0.4262188673019409\n",
            "Epoch: 26 / Loss: 0.4444718062877655\n",
            "Epoch: 27 / Loss: 0.414643794298172\n",
            "Epoch: 28 / Loss: 0.42722681164741516\n",
            "Epoch: 29 / Loss: 0.39848214387893677\n",
            "Epoch: 30 / Loss: 0.4081428647041321\n",
            "Epoch: 31 / Loss: 0.38387277722358704\n",
            "Epoch: 32 / Loss: 0.39336854219436646\n",
            "Epoch: 33 / Loss: 0.37289485335350037\n",
            "Epoch: 34 / Loss: 0.3833627700805664\n",
            "Epoch: 35 / Loss: 0.3653881847858429\n",
            "Epoch: 36 / Loss: 0.37755948305130005\n",
            "Epoch: 37 / Loss: 0.360519677400589\n",
            "Epoch: 38 / Loss: 0.3746175765991211\n",
            "Epoch: 39 / Loss: 0.35754239559173584\n",
            "Epoch: 40 / Loss: 0.37329331040382385\n",
            "Epoch: 41 / Loss: 0.3547622561454773\n",
            "Epoch: 42 / Loss: 0.3719489574432373\n",
            "Epoch: 43 / Loss: 0.35225701332092285\n",
            "Epoch: 44 / Loss: 0.37028181552886963\n",
            "Epoch: 45 / Loss: 0.34906551241874695\n",
            "Epoch: 46 / Loss: 0.36773911118507385\n",
            "Epoch: 47 / Loss: 0.345456063747406\n",
            "Epoch: 48 / Loss: 0.3642953038215637\n",
            "Epoch: 49 / Loss: 0.3411365747451782\n",
            "Epoch: 50 / Loss: 0.3601376712322235\n",
            "Epoch: 51 / Loss: 0.3368813693523407\n",
            "Epoch: 52 / Loss: 0.3563644587993622\n",
            "Epoch: 53 / Loss: 0.3330376446247101\n",
            "Epoch: 54 / Loss: 0.35307425260543823\n",
            "Epoch: 55 / Loss: 0.3295092284679413\n",
            "Epoch: 56 / Loss: 0.35025787353515625\n",
            "Epoch: 57 / Loss: 0.32637813687324524\n",
            "Epoch: 58 / Loss: 0.3475625216960907\n",
            "Epoch: 59 / Loss: 0.3234669268131256\n",
            "Epoch: 60 / Loss: 0.3453649580478668\n",
            "Epoch: 61 / Loss: 0.31937041878700256\n",
            "Epoch: 62 / Loss: 0.3409581482410431\n",
            "Epoch: 63 / Loss: 0.3154865503311157\n",
            "Epoch: 64 / Loss: 0.33779245615005493\n",
            "Epoch: 65 / Loss: 0.31215789914131165\n",
            "Epoch: 66 / Loss: 0.3349522054195404\n",
            "Epoch: 67 / Loss: 0.30943888425827026\n",
            "Epoch: 68 / Loss: 0.33250629901885986\n",
            "Epoch: 69 / Loss: 0.3070952892303467\n",
            "Epoch: 70 / Loss: 0.33098965883255005\n",
            "Epoch: 71 / Loss: 0.3050810396671295\n",
            "Epoch: 72 / Loss: 0.3287997245788574\n",
            "Epoch: 73 / Loss: 0.3027539551258087\n",
            "Epoch: 74 / Loss: 0.3265993893146515\n",
            "Epoch: 75 / Loss: 0.30033010244369507\n",
            "Epoch: 76 / Loss: 0.3242902159690857\n",
            "Epoch: 77 / Loss: 0.2979530394077301\n",
            "Epoch: 78 / Loss: 0.32196149230003357\n",
            "Epoch: 79 / Loss: 0.2956060469150543\n",
            "Epoch: 80 / Loss: 0.3202820122241974\n",
            "Epoch: 81 / Loss: 0.293682724237442\n",
            "Epoch: 82 / Loss: 0.3181564211845398\n",
            "Epoch: 83 / Loss: 0.2913345694541931\n",
            "Epoch: 84 / Loss: 0.3162412941455841\n",
            "Epoch: 85 / Loss: 0.2890273630619049\n",
            "Epoch: 86 / Loss: 0.31403055787086487\n",
            "Epoch: 87 / Loss: 0.2868228554725647\n",
            "Epoch: 88 / Loss: 0.31175535917282104\n",
            "Epoch: 89 / Loss: 0.2844349443912506\n",
            "Epoch: 90 / Loss: 0.3094162940979004\n",
            "Epoch: 91 / Loss: 0.28217294812202454\n",
            "Epoch: 92 / Loss: 0.3070700764656067\n",
            "Epoch: 93 / Loss: 0.2798509895801544\n",
            "Epoch: 94 / Loss: 0.30481386184692383\n",
            "Epoch: 95 / Loss: 0.27752628922462463\n",
            "Epoch: 96 / Loss: 0.3024044930934906\n",
            "Epoch: 97 / Loss: 0.27516791224479675\n",
            "Epoch: 98 / Loss: 0.3004188537597656\n",
            "Epoch: 99 / Loss: 0.2732396423816681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(1000)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "train(model, optimizer, criterion)"
      ],
      "metadata": {
        "id": "68HPPQ3rWIi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da62b26e-70fa-4d64-f812-b22041284298"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 / Loss: 1.4175876379013062\n",
            "Epoch: 1 / Loss: 60.62259292602539\n",
            "Epoch: 2 / Loss: 63.911434173583984\n",
            "Epoch: 3 / Loss: 38.0161018371582\n",
            "Epoch: 4 / Loss: 6.394725322723389\n",
            "Epoch: 5 / Loss: 7.909797191619873\n",
            "Epoch: 6 / Loss: 6.145054817199707\n",
            "Epoch: 7 / Loss: 2.557283639907837\n",
            "Epoch: 8 / Loss: 6.063526153564453\n",
            "Epoch: 9 / Loss: 0.1807910054922104\n",
            "Epoch: 10 / Loss: 0.7239670157432556\n",
            "Epoch: 11 / Loss: 2.9259183406829834\n",
            "Epoch: 12 / Loss: 2.4086036682128906\n",
            "Epoch: 13 / Loss: 0.6560602784156799\n",
            "Epoch: 14 / Loss: 1.468178391456604\n",
            "Epoch: 15 / Loss: 0.5164960622787476\n",
            "Epoch: 16 / Loss: 0.6601442694664001\n",
            "Epoch: 17 / Loss: 0.2470308244228363\n",
            "Epoch: 18 / Loss: 0.20085622370243073\n",
            "Epoch: 19 / Loss: 0.10099899023771286\n",
            "Epoch: 20 / Loss: 0.090593621134758\n",
            "Epoch: 21 / Loss: 0.08535785973072052\n",
            "Epoch: 22 / Loss: 0.08369014412164688\n",
            "Epoch: 23 / Loss: 0.0826132670044899\n",
            "Epoch: 24 / Loss: 0.08176982402801514\n",
            "Epoch: 25 / Loss: 0.081024669110775\n",
            "Epoch: 26 / Loss: 0.08034519106149673\n",
            "Epoch: 27 / Loss: 0.07971549034118652\n",
            "Epoch: 28 / Loss: 0.07912461459636688\n",
            "Epoch: 29 / Loss: 0.07856675982475281\n",
            "Epoch: 30 / Loss: 0.07803788781166077\n",
            "Epoch: 31 / Loss: 0.07753478735685349\n",
            "Epoch: 32 / Loss: 0.07705891877412796\n",
            "Epoch: 33 / Loss: 0.07660643011331558\n",
            "Epoch: 34 / Loss: 0.07617509365081787\n",
            "Epoch: 35 / Loss: 0.07576119899749756\n",
            "Epoch: 36 / Loss: 0.07536498457193375\n",
            "Epoch: 37 / Loss: 0.07498786598443985\n",
            "Epoch: 38 / Loss: 0.0746251568198204\n",
            "Epoch: 39 / Loss: 0.07427620142698288\n",
            "Epoch: 40 / Loss: 0.07393897324800491\n",
            "Epoch: 41 / Loss: 0.07361233979463577\n",
            "Epoch: 42 / Loss: 0.07329711318016052\n",
            "Epoch: 43 / Loss: 0.0729956403374672\n",
            "Epoch: 44 / Loss: 0.07270240038633347\n",
            "Epoch: 45 / Loss: 0.07241260260343552\n",
            "Epoch: 46 / Loss: 0.07212953269481659\n",
            "Epoch: 47 / Loss: 0.07185634970664978\n",
            "Epoch: 48 / Loss: 0.07159121334552765\n",
            "Epoch: 49 / Loss: 0.07132921367883682\n",
            "Epoch: 50 / Loss: 0.07107563316822052\n",
            "Epoch: 51 / Loss: 0.07082968205213547\n",
            "Epoch: 52 / Loss: 0.07059106230735779\n",
            "Epoch: 53 / Loss: 0.07035379856824875\n",
            "Epoch: 54 / Loss: 0.0701255351305008\n",
            "Epoch: 55 / Loss: 0.0699051171541214\n",
            "Epoch: 56 / Loss: 0.06968983262777328\n",
            "Epoch: 57 / Loss: 0.06947911530733109\n",
            "Epoch: 58 / Loss: 0.06927289813756943\n",
            "Epoch: 59 / Loss: 0.06906887143850327\n",
            "Epoch: 60 / Loss: 0.06886926293373108\n",
            "Epoch: 61 / Loss: 0.06867489218711853\n",
            "Epoch: 62 / Loss: 0.06848545372486115\n",
            "Epoch: 63 / Loss: 0.06830137968063354\n",
            "Epoch: 64 / Loss: 0.06812167912721634\n",
            "Epoch: 65 / Loss: 0.06794475764036179\n",
            "Epoch: 66 / Loss: 0.06777245551347733\n",
            "Epoch: 67 / Loss: 0.06760389357805252\n",
            "Epoch: 68 / Loss: 0.06743638217449188\n",
            "Epoch: 69 / Loss: 0.06727227568626404\n",
            "Epoch: 70 / Loss: 0.06711132824420929\n",
            "Epoch: 71 / Loss: 0.0669521614909172\n",
            "Epoch: 72 / Loss: 0.06679564714431763\n",
            "Epoch: 73 / Loss: 0.06664196401834488\n",
            "Epoch: 74 / Loss: 0.0664917379617691\n",
            "Epoch: 75 / Loss: 0.06634502112865448\n",
            "Epoch: 76 / Loss: 0.06620080769062042\n",
            "Epoch: 77 / Loss: 0.06605809181928635\n",
            "Epoch: 78 / Loss: 0.06591802090406418\n",
            "Epoch: 79 / Loss: 0.06577844172716141\n",
            "Epoch: 80 / Loss: 0.06564171612262726\n",
            "Epoch: 81 / Loss: 0.06550757586956024\n",
            "Epoch: 82 / Loss: 0.06537630409002304\n",
            "Epoch: 83 / Loss: 0.06524655967950821\n",
            "Epoch: 84 / Loss: 0.06511875987052917\n",
            "Epoch: 85 / Loss: 0.0649930089712143\n",
            "Epoch: 86 / Loss: 0.06486959755420685\n",
            "Epoch: 87 / Loss: 0.06474734842777252\n",
            "Epoch: 88 / Loss: 0.0646272599697113\n",
            "Epoch: 89 / Loss: 0.06450845301151276\n",
            "Epoch: 90 / Loss: 0.06439171731472015\n",
            "Epoch: 91 / Loss: 0.06427714973688126\n",
            "Epoch: 92 / Loss: 0.06416405737400055\n",
            "Epoch: 93 / Loss: 0.06405235826969147\n",
            "Epoch: 94 / Loss: 0.06394216418266296\n",
            "Epoch: 95 / Loss: 0.06383351981639862\n",
            "Epoch: 96 / Loss: 0.06372588127851486\n",
            "Epoch: 97 / Loss: 0.063620425760746\n",
            "Epoch: 98 / Loss: 0.06351612508296967\n",
            "Epoch: 99 / Loss: 0.06341289728879929\n"
          ]
        }
      ]
    }
  ]
}